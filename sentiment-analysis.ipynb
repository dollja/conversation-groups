{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "import pandas\n",
    "import torch\n",
    "import sklearn.metrics\n",
    "import torch.utils.data"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "c:\\Users\\dollj\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " That one hot + neural network model did not work very well!\n",
    " So now for a different\n",
    " technique that treats text as a sequence,\n",
    " this will involve recurrent\n",
    " networks, using a particular kind called an LSTM."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " Using the same dataset of sentiments on movie reviews, we\n",
    " will use a pre-trained language model from spacy.\n",
    " Using wikipedia, spacy comes pretrained with word vectors,\n",
    " which are dense encodings, so instead of one hot encoding,\n",
    " we use the word vector.\n",
    " The nice thing about this is we actually do less\n",
    " work to set up our data AND our model starts\n",
    "  with knowledge from the language\n",
    " model built over wikipedia.\n",
    " Here is an example word vector:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "for token in nlp('hello'):\n",
    "    print(token)\n",
    "    print(token.vector)\n",
    "    print(token.vector.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "hello\n",
      "[ 2.2407e+00  1.0389e+00  1.3092e+00 -1.7335e+00 -7.8466e-01 -2.9269e-01\n",
      " -1.8059e+00 -2.5223e+00  7.8025e-01  2.4899e+00 -9.1849e-02  2.8755e-01\n",
      " -1.5057e+00  2.6337e+00  2.5252e+00 -2.2432e-01 -2.2068e+00 -5.7895e-01\n",
      " -5.6551e-01 -1.9338e+00  1.4973e+00  8.5889e-01  3.3559e+00 -3.7527e+00\n",
      "  2.2585e-01 -1.6969e-01  5.1389e-01  4.6073e-01 -2.8248e-01 -2.6048e+00\n",
      " -3.5896e+00 -1.0971e+00 -1.5517e+00 -1.2185e-01  2.8633e+00 -1.2525e+00\n",
      " -1.6924e+00 -2.2917e+00  9.7793e-01  4.6954e-01 -3.5950e+00 -1.7357e-01\n",
      "  9.8050e-01 -1.8044e+00 -7.2183e-01 -4.0709e-01 -3.0943e+00  1.3095e-01\n",
      " -2.9015e+00  1.4768e+00 -1.0588e+00 -2.8123e+00  1.2936e+00 -7.5977e-03\n",
      "  2.9975e+00 -2.4438e+00  1.2348e-01  1.8322e+00  3.5869e-01 -1.8335e-02\n",
      "  1.9534e+00  1.4417e+00  9.9895e-01 -2.8209e+00 -7.5846e-01 -1.8438e+00\n",
      " -3.2658e+00 -4.6574e-01  9.0322e-01  7.9868e-01 -1.6134e+00 -3.3082e-01\n",
      "  1.1541e+00 -4.7334e+00  1.4964e+00 -2.4014e+00 -1.3461e+00 -9.5551e-01\n",
      "  2.9671e-01 -1.4506e+00 -8.7128e-01 -3.0714e+00  1.3597e+00 -3.8133e-02\n",
      "  1.6414e+00 -9.0879e-01  2.7406e+00  2.2951e+00 -3.1423e+00 -3.7525e+00\n",
      "  7.4033e-01  1.4921e+00  4.7422e-01 -1.8337e+00 -1.8168e+00  6.6901e-01\n",
      " -1.3612e+00 -2.2729e+00 -1.7656e+00 -7.3968e-01 -1.2859e-01  8.7759e-01\n",
      "  1.4448e+00 -2.7825e+00 -7.9597e-01  6.8845e-02  2.4634e+00  6.3549e-01\n",
      " -2.4446e+00  2.7566e+00 -2.9590e+00 -2.9554e+00  2.4260e+00 -5.0678e-01\n",
      " -1.8003e-01 -1.5840e-01  1.5381e+00 -3.0552e+00 -1.5079e-01 -1.2797e+00\n",
      " -2.3727e+00 -8.5434e-01 -8.0659e-01  1.4002e+00  3.3321e-01  1.4099e-02\n",
      "  5.0187e-02  5.4004e-01 -2.5711e-01 -3.7119e-01  5.1628e-01  4.6177e-01\n",
      " -9.6974e-01 -1.9037e+00  1.3771e+00 -4.2902e+00  4.4036e-01 -3.7129e+00\n",
      "  3.6913e-01  6.0736e-01 -1.2517e+00  5.4799e-01  1.7881e+00 -1.3292e-02\n",
      " -1.2324e+00 -5.6894e-01  4.0923e-01 -1.4679e+00 -5.0147e-02  4.7829e-02\n",
      "  3.9354e-01 -8.4717e-02 -2.7886e-01 -1.6172e+00 -3.1956e+00  8.2014e-01\n",
      "  1.0563e-01  3.6849e-01 -5.9515e-01 -1.3946e+00  1.8662e+00 -1.2534e+00\n",
      " -2.9180e+00  5.5685e-01  1.5468e+00  6.2468e-02 -1.4367e-01  1.4024e+00\n",
      " -2.1279e+00 -1.0165e+00  2.3544e+00 -5.7429e-02  2.9195e+00  3.7250e-01\n",
      "  2.0609e+00 -7.1026e-01 -3.9626e-01  2.9378e+00  6.4680e-01  9.5497e-01\n",
      " -7.8183e-01  2.3416e+00  2.0079e-01  5.9501e-01  7.5665e-01  4.0957e+00\n",
      "  3.3441e+00 -3.7983e+00  5.2220e-01  1.4794e+00 -7.8054e-01  3.2953e+00\n",
      " -3.5794e+00  1.2550e+00  7.0703e-01  3.5257e-01 -6.7321e-01  1.7639e+00\n",
      "  4.5984e-01  3.0588e+00 -1.6329e+00 -1.7406e-01 -1.4539e-01  5.3963e-01\n",
      " -2.1813e+00  7.3330e-02  3.0116e+00 -1.1945e+00 -5.4978e-01 -1.7296e+00\n",
      "  2.7608e+00 -7.8110e-01  2.5770e+00 -2.0205e+00  9.5212e-01 -2.6341e-02\n",
      "  7.1217e-01 -1.7927e-01 -5.0474e-01 -1.9870e-01 -2.3128e+00 -4.7594e-01\n",
      "  1.0176e+00 -1.4798e+00  3.5564e-01  1.8736e+00  1.1265e+00 -4.2133e-01\n",
      " -4.9311e-01  5.0992e-01 -5.6429e+00 -1.0555e+00 -2.2193e+00 -2.0264e+00\n",
      " -1.7333e+00 -2.7713e+00 -8.4593e-01  9.7533e-01  3.6652e-01  5.5371e-01\n",
      "  2.4250e-01  7.4996e-01  1.5628e+00  1.0085e+00  2.5170e+00 -1.2660e+00\n",
      "  9.3572e-01 -7.5181e-01  2.4042e+00 -2.9900e-01 -1.3888e+00  4.5666e-02\n",
      " -2.8199e+00 -8.0165e-02 -3.5760e-01 -3.4620e+00 -4.3997e-03  1.1915e+00\n",
      " -2.0684e+00  1.4717e+00  5.9559e-01 -4.0306e+00 -3.3536e+00 -1.7596e+00\n",
      "  3.2136e-01 -1.6446e-01 -1.2610e+00 -1.4749e+00  2.5708e-01  1.4584e+00\n",
      "  1.0947e+00  3.7450e-01 -6.4695e-01 -2.1451e-01  7.1111e-01 -3.2445e-01\n",
      "  1.4885e+00 -2.6306e+00 -7.8051e-01  4.1903e-01 -4.8808e+00  1.5290e+00\n",
      " -9.3572e-01 -2.8508e+00 -8.5380e-01  1.6754e+00  2.2445e+00  1.7918e-01\n",
      " -3.5615e-01  1.7458e+00  2.4266e-01 -5.5352e-01  3.4471e+00 -2.3453e+00\n",
      "  1.7419e+00 -3.1414e+00 -4.2509e-01  3.0228e+00 -7.2594e-01  2.1592e-01]\n",
      "(300,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " And for the dataset, we just extract the vectors as tensors\n",
    " and return the length of each string in tokens.\n",
    " This is important for working with pytorch recurrent networks."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = pandas \\\n",
    "            .read_csv('C:\\github\\pytorch-dl7\\sentiment.tsv', sep='\\t', header=0) \\\n",
    "            .groupby('SentenceId') \\\n",
    "            .first()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if type(idx) is torch.Tensor:\n",
    "            idx = idx.item()\n",
    "        sample = self.data.iloc[idx]\n",
    "        token_vectors = []\n",
    "        # switching off NER for a tiny speed boost\n",
    "        for token in nlp(sample.Phrase.lower(), disable=['ner']):\n",
    "            token_vectors.append(token.vector)\n",
    "\n",
    "        # tokens and length as inputs -- the length\n",
    "        # is needed to 'pack' variable length sequences\n",
    "        # output is the sentiment score \n",
    "        return (torch.tensor(token_vectors),\n",
    "                torch.tensor(len(token_vectors)),\n",
    "                torch.tensor(sample.Sentiment))\n",
    "\n",
    "\n",
    "sentiment = SentimentDataset()\n",
    "sentiment[0]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-4-e8c5ee0f4013>:24: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n",
      "  return (torch.tensor(token_vectors),\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[ -9.3629,   9.2761,  -7.2708,  ...,   2.6801,  -6.8160,   3.5737],\n",
       "         [ -2.9056,  -2.5910,   3.3317,  ...,  -7.2864,  -3.5043,  -2.2919],\n",
       "         [-12.6670,  -6.5680,  -0.6154,  ...,  -8.0021,  -0.3171,  -7.7062],\n",
       "         ...,\n",
       "         [ -9.3629,   9.2761,  -7.2708,  ...,   2.6801,  -6.8160,   3.5737],\n",
       "         [  2.7879,  -2.6493,  -0.4601,  ...,  -0.6575,  -5.5132,   0.4322],\n",
       "         [ -0.0765,  -4.6896,  -4.0431,  ...,   1.3040,  -0.5270,  -1.3622]]),\n",
       " tensor(37),\n",
       " tensor(1))"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# break this into a training and testing dataset, and need\n",
    "# to collate into fixed width as these will be\n",
    "# variable batches\n",
    "def collate(batch):\n",
    "    # sort indescending length order -- this is needed for\n",
    "    # padding seqeunces in pytorch\n",
    "    batch.sort(key=lambda x: x[1], reverse=True)\n",
    "    sequences, lengths, sentiments = zip(*batch)\n",
    "    sequences = torch.nn.utils.rnn.pad_sequence(\n",
    "        sequences, batch_first=True\n",
    "    )\n",
    "    sentiments = torch.stack(sentiments)\n",
    "    lengths = torch.stack(lengths)\n",
    "    return sequences, lengths, sentiments\n",
    "\n",
    "\n",
    "number_for_testing = int(len(sentiment) * 0.05)\n",
    "number_for_training = len(sentiment) - number_for_testing\n",
    "train, test = torch.utils.data.random_split(sentiment,\n",
    "                                            [number_for_training, number_for_testing])\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    train, batch_size=32, shuffle=True,\n",
    "    collate_fn=collate)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    test, batch_size=32, shuffle=True,\n",
    "    collate_fn=collate)\n",
    "\n",
    "# take a peek and see what we are collating\n",
    "for batch in trainloader:\n",
    "    print(batch[0].shape, batch[1].shape, batch[2].shape)\n",
    "    # what is the max length?\n",
    "    print(batch[1][0])\n",
    "    break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([32, 38, 300]) torch.Size([32]) torch.Size([32])\n",
      "tensor(38)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " Now, this is still a regression problem, but instead of\n",
    " one hot encoded words and a plain nerual network,\n",
    " we will have sequences\n",
    " of word vectors, from the learned wikipedia model.\n",
    " These sequences in turn will be * packed * which\n",
    " is because they all have different lengths,\n",
    " run through the recurrent network\n",
    " which loops word vector by word vector to compute a final\n",
    " numerical representation of the whole sequence - -just like\n",
    " reading - -word for word in order.\n",
    " This is why we need the sequence lengths, you need to know\n",
    " the boundaries on which to pack."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dimensions, size=128, layers=1):\n",
    "        super().__init__()\n",
    "        self.seq = torch.nn.LSTM(input_dimensions, size, layers)\n",
    "        self.layer_one = torch.nn.Linear(size * layers, size)\n",
    "        self.activation_one = torch.nn.ReLU()\n",
    "        self.layer_two = torch.nn.Linear(size, size)\n",
    "        self.activation_two = torch.nn.ReLU()\n",
    "        self.shape_outputs = torch.nn.Linear(size, 5)\n",
    "\n",
    "    def forward(self, inputs, lengths):\n",
    "        # need to sort the sequences for pytorch -- which we\n",
    "        # did in our collation above\n",
    "        number_of_batches = lengths.shape[0]\n",
    "        packed_inputs = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            inputs,\n",
    "            lengths,\n",
    "            batch_first=True)\n",
    "        buffer, (hidden, cell) = self.seq(packed_inputs)\n",
    "        # batch first...\n",
    "        buffer = hidden.permute(1, 0, 2)\n",
    "        # flatten out the last hidden state -- this will\n",
    "        # be the tensor representing each batch\n",
    "        buffer = buffer.contiguous().view(number_of_batches, -1)\n",
    "        # and feed along to a simple output network with\n",
    "        # a single output cell for regression\n",
    "        buffer = self.layer_one(buffer)\n",
    "        buffer = self.activation_one(buffer)\n",
    "        buffer = self.layer_two(buffer)\n",
    "        buffer = self.activation_two(buffer)\n",
    "        buffer = self.shape_outputs(buffer)\n",
    "        return buffer\n",
    "\n",
    "\n",
    "# get the input dimensions from the first sample\n",
    "# encodings are word, vectors - so index 1 at the end\n",
    "model = Model(sentiment[0][0].shape[1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "model.train()\n",
    "for epoch in range(64):\n",
    "    losses = []\n",
    "    for sequences, lengths, sentiments in tqdm.tqdm(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        results = model(sequences, lengths)\n",
    "        loss = loss_function(results, sentiments)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Loss: {0}\".format(torch.tensor(losses).mean()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:46<00:00,  5.51it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 1.4734059572219849\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:45<00:00,  5.60it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 1.3330905437469482\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:46<00:00,  5.49it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 1.2441843748092651\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:46<00:00,  5.51it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 1.1678261756896973\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:45<00:00,  5.52it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 1.0837353467941284\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:46<00:00,  5.47it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.9996069073677063\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:47<00:00,  5.36it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.9185651540756226\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:44<00:00,  5.76it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.8219011425971985\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:43<00:00,  5.83it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.7521815299987793\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:43<00:00,  5.84it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.6641423106193542\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:45<00:00,  5.55it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.5820595026016235\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:46<00:00,  5.41it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.5267065167427063\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:46<00:00,  5.49it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.4469405710697174\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:47<00:00,  5.40it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.4057372808456421\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:46<00:00,  5.42it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.37197065353393555\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:45<00:00,  5.54it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.35849249362945557\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:47<00:00,  5.30it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.29713374376296997\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:46<00:00,  5.44it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.25557243824005127\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:45<00:00,  5.61it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.25849229097366333\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:45<00:00,  5.57it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.2347264587879181\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:43<00:00,  5.78it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.20148402452468872\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:43<00:00,  5.83it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.19072909653186798\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:43<00:00,  5.81it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.20309260487556458\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:43<00:00,  5.83it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.16435274481773376\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:43<00:00,  5.82it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.1608603596687317\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:50<00:00,  5.04it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.1625564992427826\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:49<00:00,  5.09it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.13566236197948456\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:49<00:00,  5.18it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.1588263362646103\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:49<00:00,  5.13it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.15672093629837036\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:49<00:00,  5.14it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.14521686732769012\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:51<00:00,  4.98it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.1399264931678772\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:48<00:00,  5.22it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.13188566267490387\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:44<00:00,  5.69it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.1885973960161209\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:44<00:00,  5.75it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.12250088155269623\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:44<00:00,  5.67it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.08807788789272308\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:44<00:00,  5.74it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.0928792729973793\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:44<00:00,  5.74it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.13396723568439484\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:44<00:00,  5.77it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.1313169151544571\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:44<00:00,  5.75it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.10199742019176483\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:44<00:00,  5.69it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.06540792435407639\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:44<00:00,  5.77it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.06976909935474396\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:43<00:00,  5.79it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.14090768992900848\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:43<00:00,  5.79it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.13151879608631134\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:43<00:00,  5.79it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.09350687265396118\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:43<00:00,  5.83it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.1081337183713913\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:43<00:00,  5.81it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.07678601890802383\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:43<00:00,  5.82it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.11498833447694778\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:43<00:00,  5.81it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.10333216190338135\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:43<00:00,  5.79it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.07117589563131332\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:43<00:00,  5.80it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.04350518807768822\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:43<00:00,  5.81it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.08667664229869843\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:43<00:00,  5.79it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.10806074738502502\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:43<00:00,  5.79it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.11067166179418564\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [11:12:08<00:00, 158.77s/it]      \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.08212721347808838\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:44<00:00,  5.76it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.0676676332950592\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:43<00:00,  5.85it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.05687376484274864\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:44<00:00,  5.77it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.09348174929618835\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:49<00:00,  5.10it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.08590392768383026\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:49<00:00,  5.11it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.10759774595499039\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:47<00:00,  5.37it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.09520349651575089\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:46<00:00,  5.41it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.05474971607327461\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:50<00:00,  5.08it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.05004001408815384\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:47<00:00,  5.31it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.06291337311267853\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 254/254 [00:47<00:00,  5.31it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 0.11179642379283905\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "results_buffer=[]\n",
    "actual_buffer=[]\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for test_seq, test_len, test_sentiment in testloader:\n",
    "        results=model(test_seq, test_len).argmax(dim=1).numpy()\n",
    "        results_buffer.append(results)\n",
    "        actual_buffer.append(test_sentiment)\n",
    "\n",
    "print(sklearn.metrics.classification_report(\n",
    "    np.concatenate(actual_buffer),\n",
    "    np.concatenate(results_buffer)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.33      0.34        48\n",
      "           1       0.44      0.43      0.43       112\n",
      "           2       0.32      0.31      0.31        91\n",
      "           3       0.42      0.48      0.45       112\n",
      "           4       0.51      0.43      0.47        63\n",
      "\n",
      "    accuracy                           0.41       426\n",
      "   macro avg       0.41      0.40      0.40       426\n",
      "weighted avg       0.41      0.41      0.41       426\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 }
}